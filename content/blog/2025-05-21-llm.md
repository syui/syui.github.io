+++
date = "2025-05-21"
tags = ["openai", "llm", "mcp", "chatgpt"]
title = "mcpとchatgpt"
+++

mcp(model context protocol)が最近話題になっています。今回はこのmcpについての解説になります。

mcpは多くの人にとって学ぶ必要がない技術です。例えば、プログラマだったり、アプリ開発、フロントエンド、デザインをしているとかだと、mcpは必要ありません。mcpを学ぶ必要があるのは主にバックエンドの人です。

ということで、mcpについて解説していきます。

mcpは、今後バックエンドの主流になると思います。ようはサーバーを扱ったり、環境を構築したりといった分野。

それ以外は、主にclient、ollamaだったり、llm studioだったりのほうが便利だと思います。

mcpはAI環境の構築を目指す技術で、それぞれのmodelは今まで独自の仕様で動かしていました。これだとAIで何をするにも面倒です。したがって、AI(model)のinput/outputの形式を統一しようというのがmcpです。protocolなので、普通の人は学ぶ必要がありません。通常の開発でも意識するような領域ではないですね。

そのうち、便利なツールがでてくると思うので、それを使うのが一番かなと思います。k8sやdockerのようなものが出てくるのではないでしょうか。まあ、ollamaやllm studioがそれらに相当するかもしれませんけど。

## mcpの使い方

mcpでどうやって環境を構築するのか、具体的な手順を見ていきたいと思います。

これは何を作りたいかによりますが、基本的なmodelの呼び出しから。

### 最も簡単

```sh
pip install mcp
```

```py:server.py
from mcp.server.fastmcp import FastMCP

mcp = FastMCP(
    name="count_r",
    host="127.0.0.1",
    port=5000,
    timeout=30
)

if __name__ == "__main__":
    print("Starting MCP server...")
    mcp.run()
```

```sh
python server.py
```

### 確実な方法

```sh
git clone https://github.com/microsoft/MCP.git
cd MCP
python -m venv .venv

if [[ "$OSTYPE" == "msys" ]] || [[ "$OSTYPE" == "win32" ]] || [[ "$OSTYPE" == "cygwin" ]]; then
    # Windows (Git Bash, MSYS, Cygwin など)
    .venv\Scripts\activate
else
    # macOS/Linux (Unix系)
    .venv/bin/activate
fi

pip install -e .
pip install openai
```

```py:cli.py
# cli.py
def main():
    print("Hello MCP!")
```

```py:setup.py
from setuptools import setup

setup(
    name='mcp',
    version='0.1.0',
    py_modules=['cli'],
    entry_points={
        'console_scripts': [
            'mcp = cli:main',
        ],
    },
)
```

```py:scripts/ask.py
# scripts/ask.py
import os
import json
import httpx
import openai

from context_loader import load_context_from_repo
from prompt_template import PROMPT_TEMPLATE

PROVIDER = os.getenv("PROVIDER", "ollama")  # "ollama" or "openai"

# Ollama用
OLLAMA_HOST = os.getenv("OLLAMA_HOST", "http://localhost:11434")
OLLAMA_URL = f"{OLLAMA_HOST}/api/generate"
OLLAMA_MODEL = os.getenv("MODEL", "syui/ai")

# OpenAI用
OPENAI_BASE = os.getenv("OPENAI_API_BASE", "https://api.openai.com/v1")
OPENAI_KEY = os.getenv("OPENAI_API_KEY", "")
OPENAI_MODEL = os.getenv("MODEL", "gpt-4o-mini")

def ask_question(question, repo_path="."):
    context = load_context_from_repo(repo_path)
    prompt = PROMPT_TEMPLATE.format(context=context[:10000], question=question)

    if PROVIDER == "ollama":
        payload = {
            "model": OLLAMA_MODEL,
            "prompt": prompt,
            "stream": False
        }
        response = httpx.post(OLLAMA_URL, json=payload, timeout=60.0)
        result = response.json()
        return result.get("response", "返答がありませんでした。")

    elif PROVIDER == "openai":
        import openai
        openai.api_key = OPENAI_KEY
        openai.api_base = OPENAI_BASE

        client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        response = client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=[{"role": "user", "content": prompt}]
        )
        return response.choices[0].message.content

    else:
        return f"❌ 未知のプロバイダです: {PROVIDER}"


if __name__ == "__main__":
    import sys
    question = " ".join(sys.argv[1:])
    answer = ask_question(question)
    print("\n🧠 回答:\n", answer)
```

```py:scripts/context_loader.py
# scripts/context_loader.py
import os

def load_context_from_repo(repo_path: str, extensions={".rs", ".toml", ".md"}) -> str:
    context = ""
    for root, dirs, files in os.walk(repo_path):
        for file in files:
            if any(file.endswith(ext) for ext in extensions):
                with open(os.path.join(root, file), "r", encoding="utf-8", errors="ignore") as f:
                    content = f.read()
                    context += f"\n\n# FILE: {os.path.join(root, file)}\n{content}"
    return context
```

```py:scripts/prompt_template.py
#scripts/prompt_template.py
PROMPT_TEMPLATE = """
あなたは優秀なAIアシスタントです。

以下のコードベースの情報を参考にして、質問に答えてください。

[コードコンテキスト]
{context}

[質問]
{question}
"""
```

```sh
ollama run syui/ai
---
mcp scripts/ask.py
python scripts/ask.py "このプロジェクトで設定ファイルを読み込む処理はどこ？"
```

`PROMPT_TEMPLATE`の記述はollamaのmodelなどで異なる場合があります。

mcpは自動化の環境構築に最適だと思います。
