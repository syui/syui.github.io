+++
date = "2025-05-13"
tags = ["lmstudio", "ollama", "openai", "qwen"]
title = "qwen3が4oと同じくらい使える"
+++

アリババが出しているqwen3がopenaiの4oと同じくらい使えるので、chatgptに課金している人は、local llmで`qwen3`に移行してみてもいいかもしれません。

https://huggingface.co/Qwen/Qwen3-30B-A3B-Base

今回は、local llmに移行する方法をいくつか紹介します。local llmとは、簡単に言うと自分のpcやserverでAI modelを動かすことです。

## lmstudioとollama

local llmを使うには、[lmstudio](https://lmstudio.ai/)や[ollama](https://ollama.com/)がおすすめです。

おそらく、`ollama`のほうが`serve`を起動しやすいのでおすすめです。`localhost:11434`

```sh
$ ollama serve
```

もちろん、`lmstudio`もserver機能はあります。

https://lmstudio.ai/docs/app/api

editorやios client連携もここにアクセスして使います。

アドレスバーからmodelを選択します。少し分かりづらいかもしれません。

## custom model

日本語の小説やロールプレイに特化したqwen3のcustom modelが公開されています。

> NSFW(Not Safe For Work)は職場での閲覧に適さないコンテンツを意味します。ERP(Erotic Role Play)は主にロールプレイを意味します。

- https://huggingface.co/Aratako/Qwen3-30B-A3B-NSFW-JP
- https://huggingface.co/Aratako/Qwen3-30B-A3B-ERP-v0.1

```sh
# https://huggingface.co/Aratako/Qwen3-30B-A3B-ERP-v0.1
$ ollama run hf.co/Aratako/Qwen3-30B-A3B-ERP-v0.1-GGUF

/set system ${string} <設定が可能です>
/save ${session} <コマンドで現在のチャット履歴を名前付きファイルに保存できます>
/load ${session} <以前保存した会話履歴を復元し、継続した対話が可能です>
```

modelの選定→データセット準備→finetuning(ms-swiftやTransformersなど)→modelの変換や公開という流れで構築できます。

1. ベースモデルの選定
Qwen3の公式リリースモデルをHugging FaceやGitHubからダウンロードします。

2. データセットの準備
目的に応じた日本語や特定ジャンルのテキストデータを収集します。

質問応答、会話、ロールプレイ、専門知識など用途に合わせてデータを整形します。

3. ファインチューニングの実施
フレームワーク選択: Megatron-LMベースの「ms-swift」や、Hugging Face Transformersなどが利用可能です。

- 学習用データセットを用意し、コマンドラインで学習ジョブを実行。
- 大規模GPU(例: H200x8台など)を使うことで効率的に学習が進みます。
- 学習後はモデルをmcore形式からHF(Hugging Face)形式に変換し、配布やローカル利用が可能です。

```sh
# 学習コマンド例（ms-swift/Megatron-LM）
# https://github.com/modelscope/ms-swift
#!/bin/bash
swift train \
  --model Qwen3-30B-A3B \
  --data /path/to/your/dataset \
  --output /path/to/output/dir \
  --epochs 3 \
  --batch-size 64 \
  --lr 1e-5
```

> HPC-AIという謎のクラウドGPUサービスを使用しました。なんとH200x8が$16.72/hourという異常な安さで利用できます。

https://zenn.dev/aratako_lm/articles/90c81270ef64bf

## safetensors -> gguf

ollamaなどで使うには`.gguf`に変換しなければなりません。

```sh
$ git clone https://github.com/ggerganov/llama.cpp
$ cd llama.cpp
$ pip install -r requirements.txt

$ vim convert_hf_to_gguf_update.py
models = [{"name": "qwen3",        "tokt": TOKENIZER_TYPE.SPM, "repo": "https://huggingface.co/Qwen/Qwen3-30B-A3B-Base"}]

$ python convert_hf_to_gguf_update.py <huggingface_token>
# model download: /path/to/model_dir
$ python convert-hf-to-gguf.py /path/to/model_dir --outtype q8_0 --outfile /path/to/output/model.gguf
```

```sh
$ vim Modelfile
FROM ./model.gguf

$ ollama create qwen3-custom -f ./Modelfile
$ ollama run qwen3-custom
```

## client ["ios", "mac" ]

ollama用のclientです。他のapiにも対応しているのかはわかりません。

https://github.com/gluonfield/enchanted

まずlocal networkで使用する場合は`OLLAMA_ORIGINS`だけ設定してください。

```sh
set OLLAMA_ORIGINS=192.168.1.*
```

次にwanから使用する場合は、[tailscale](https://tailscale.com/), [ngrok](https://ngrok.com/), [cloudflare tunnel](https://developers.cloudflare.com/cloudflare-one/connections/connect-networks/)などを使用します。`OLLAMA_HOST`を設定します。

```sh
$ set OLLAMA_HOST=0.0.0.0
```

そして、ollamaを起動します。

```sh
$ ollama serve
```

ios appの設定には`192.168.1.x:11434`を入れます。アドレスバーからmodelを選択します。

